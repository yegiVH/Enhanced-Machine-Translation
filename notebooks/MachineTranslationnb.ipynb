{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbcb8c",
   "metadata": {},
   "source": [
    "This project implements a naive English→French translation model using word embeddings.\n",
    "It includes both a nearest-neighbor baseline and an aligned embedding model using Procrustes optimization.\n",
    "The goal is to explore vector-space semantics and evaluate how well word-level translation can work without large neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd56ad",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle #  used for serializing and deserializing Python object structures\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Resources (Embeddings & Seed Dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\")) # {word : array([])}\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5600fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read the CSV file into a pandas DataFrame using a space as the delimiter  \n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')  \n",
    "\n",
    "    # Initialize an empty dictionary to store the English to French translations  \n",
    "    etof = {}  \n",
    "\n",
    "    # Loop through each row in the DataFrame  \n",
    "    for i in range(len(my_file)):  \n",
    "        # Access the English word from the first column (index 0) of the current row  \n",
    "        en = my_file.loc[i][0]  \n",
    "        \n",
    "        # Access the French translation from the second column (index 1) \n",
    "        # of the current row  \n",
    "        fr = my_file.loc[i][1]  \n",
    "        \n",
    "        # Add the English word and its French translation to the dictionary  \n",
    "        etof[en] = fr  \n",
    "\n",
    "    # Return the populated dictionary containing English to French translations  \n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "\n",
    "en_fr_train = get_dict('./data/en-fr.train.txt') # key: English word, value: French word\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "\n",
    "en_fr_test = get_dict('./data/en-fr.test.txt') # {english:french}\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5a5e3",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3dbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    # you have to set this variable to the true label.\n",
    "    cos = -10    \n",
    "    dot = np.dot(A, B)\n",
    "    normb = np.linalg.norm(B)\n",
    "    \n",
    "    if len(A.shape) == 1: # If A is just a vector, we get the norm\n",
    "        norma = np.linalg.norm(A)\n",
    "        cos = dot / (norma * normb)\n",
    "    else: # If A is a matrix, \n",
    "        # then compute the norms of the word vectors of the matrix (norm of each row)\n",
    "        norma = np.linalg.norm(A, axis=1)\n",
    "        epsilon = 1.0e-9 # to avoid division by 0\n",
    "        cos = dot / (norma * normb + epsilon)\n",
    "        \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987fa03",
   "metadata": {},
   "source": [
    "## 3. Nearest-Neighbor Baseline (Word→Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "271500d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    similarity_l = []\n",
    "    \n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(row, v)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "\n",
    "    \n",
    "    # sort the similarity list and get the indices of the sorted list (ascending)    \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # Reverse the order of the sorted_ids array (descending)\n",
    "    sorted_ids = sorted_ids [::-1]\n",
    "    \n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[:k]\n",
    "  \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01505e81",
   "metadata": {},
   "source": [
    "## 4. Embedding Alignment (EN→FR) — Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Creates matrices of word embeddings for English and \n",
    "    French words that are mapped to each other.\n",
    "    \n",
    "    Inputs:\n",
    "        en_fr: Dictionary mapping English words to French words.\n",
    "        french_vecs: Dictionary of French word embeddings.\n",
    "        english_vecs: Dictionary of English word embeddings.\n",
    "    \n",
    "    Outputs: \n",
    "        X: Matrix with each row being the embedding of an English word. \n",
    "        Shape is (number_of_words, embedding_size).\n",
    "        \n",
    "        Y: Matrix with each row being the embedding of the corresponding French word. \n",
    "        Shape matches X.\n",
    "\n",
    "    \"\"\"\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # getting the english words (the keys in the dictionary) and storing them in a set()\n",
    "    english_set = set(english_vecs.keys())\n",
    "\n",
    "    # getting the french words (keys in the dictionary) and storing in a set()\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding \n",
    "        # and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.vstack(X_l) # stack arrays in sequence vertically (row wise)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X -> number of samples\n",
    "    m = X.shape[0]\n",
    "        \n",
    "    # diff is XR - Y    \n",
    "    diff = np.dot(X, R) - Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference    \n",
    "    diff_squared = np.square(diff)\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i is the sum_diff_squared divided by the number of examples (m)\n",
    "    loss = sum_diff_squared / m\n",
    "   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from \n",
    "        English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    # m is the number of rows in X => number of samples\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m    \n",
    "    gradient = np.dot(X.T, np.dot(X,R) - Y) * (2/m)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3502e",
   "metadata": {},
   "source": [
    "##### Finding the optimal R with Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003, verbose=True, compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    \n",
    "    # At first R is initialized with random numbers\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "    \n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate*gradient # R = R - alpha*g\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd81129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(english_word, en_embeddings, fr_embeddings, translation_matrix):\n",
    "    \"\"\"\n",
    "    Translate an English word to French using embeddings and a trained translation matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    english_word (str): The word in English to translate.\n",
    "    en_embeddings (dict): English word embeddings (word -> vector).\n",
    "    fr_embeddings (dict): French word embeddings (word -> vector).\n",
    "    translation_matrix (np.ndarray): The trained translation matrix (from English to French).\n",
    "    \n",
    "    Returns:\n",
    "    str: The translated French word.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the embedding for the English word\n",
    "    if english_word in en_embeddings:\n",
    "        en_word_embedding = en_embeddings[english_word]\n",
    "    else:\n",
    "        return f\"Word '{english_word}' not found in the English embeddings.\"\n",
    "\n",
    "    # Step 2: Apply the translation matrix to get the French embedding\n",
    "    translated_embedding = np.dot(en_word_embedding, translation_matrix)\n",
    "\n",
    "    # Step 3: Find the closest French word in the embedding space\n",
    "    min_distance = float('inf')\n",
    "    french_word = None\n",
    "\n",
    "    for fr_word, fr_embedding in fr_embeddings.items():\n",
    "        distance = np.linalg.norm(translated_embedding - fr_embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            french_word = fr_word\n",
    "\n",
    "    return french_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96403ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set:\n",
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, \n",
    "                                en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Transformation matrix R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e95d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_w = input(\"Enter the English word: \")\n",
    "french_w = translate_word(english_w, en_embeddings_subset, fr_embeddings_subset, R_train)\n",
    "print('English word:', english_w) \n",
    "print('French word:', french_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2925b",
   "metadata": {},
   "source": [
    "## 5. Evaluation (Accuracy / Top-k / Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    # The prediction is X times R\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i';\n",
    "        # also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i], Y, 1)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' \n",
    "    # (also number of rows in X)\n",
    "    accuracy = num_correct / X.shape[0]\n",
    "\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how is your translation mechanism working on the unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  \n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da84890",
   "metadata": {},
   "source": [
    "## 6. Sentence Stub (Optional for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f691dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code: very simple word-by-word sentence translation; note limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24290e",
   "metadata": {},
   "source": [
    "## 7. sacreBLEU Setup (Optional) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code: install/use sacrebleu; compute BLEU on a tiny sentence set (later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08ccfe",
   "metadata": {},
   "source": [
    "## 8. Error Analysis & Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12655f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54d2486a",
   "metadata": {},
   "source": [
    "## 9. Roadmap / Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "790d808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text: what you'll add next (transformer baseline, UI, subwords, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
